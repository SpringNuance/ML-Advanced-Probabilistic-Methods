{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42130e852daf4be6e4d78483b962cfd",
     "grade": false,
     "grade_id": "cell-5b335005bb36ae92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (Spring 2023)\n",
    "\n",
    "Pekka Marttinen, Vishnu Raj, Antti Pöllänen, Nikitin Alexander, Sebastiaan De Peuter, Tommi Gröhn, Julien Martinelli, Ali Khoshvishkaie, Onur Poyraz\n",
    "\n",
    "\n",
    "## Exercise 5, due on Tuesday March 7 at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: EM for missing observations\n",
    "2. Problem 2: Extension of 'simple example' from the lecture\n",
    "3. Problem 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c12b98afa6a333b6b4717029202b7d",
     "grade": false,
     "grade_id": "cell-298bb2ed1de6d806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ write the complete data log-likelihood $\\ell(\\rho)$\n",
    "\n",
    "Complete data refers to a data that contains all the information that is needed to perform a statistical analysis. This means that every observation in the data set has values for all the variables of interest, and there are no missing variables. Therefore, the complete data log likelihood of $\\mathbf{x}$ based on $\\rho$ is:\n",
    "\n",
    "$\\ell(\\rho) = \\sum^3_{i=1} \\log p(\\textbf{x}_i|\\rho)$, where $\\mathbf{x}_2$ is treated as complete instead of missing\n",
    "\n",
    "\n",
    "$=> \\ell(\\rho) = \\sum^3_{i=1} \n",
    "\\log \\left(\n",
    "\\dfrac{1}{\\sqrt{\\det(2 \\pi \\boldsymbol{\\Sigma})}} \\exp\\left(-\\dfrac{1}{2}(\\mathbf{x}_i - \\mu)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu)\\right)\n",
    "\\right)$\n",
    "\n",
    "$=> \\ell(\\rho) = \\sum^3_{i=1} \n",
    "\\log \\left(\n",
    "\\det(2 \\pi \\boldsymbol{\\Sigma})^{-\\frac{1}{2}} \\exp\\left(-\\dfrac{1}{2}(\\mathbf{x}_i - 0)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - 0)\\right)\n",
    "\\right)$\n",
    "\n",
    "$=> \\ell(\\rho) = \\sum^3_{i=1} \n",
    "\\log \\left(\\det(2 \\pi \\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\right)\n",
    "+ \\log \\exp \\left(-\\dfrac{1}{2}\\mathbf{x}_i^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x}_i\\right)\n",
    "$\n",
    "\n",
    "$=> \\ell(\\rho) = \\sum^3_{i=1} \n",
    "\\left({-\\dfrac{1}{2}} \\log \\left(\\det(2 \\pi \\boldsymbol{\\Sigma})\\right)\n",
    " -\\dfrac{1}{2}\\mathbf{x}_i^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x}_i\\right)\n",
    "$\n",
    "\n",
    "Observing that the term $-\\dfrac{1}{2} \\log (\\det(2 \\pi \\boldsymbol{\\Sigma}))$ is independent of $\\mathbf{x}_i$, so the log likelihood can be rephrased as:\n",
    "\n",
    "$=> \\ell(\\rho)  = -\\dfrac{3}{2} \\log \\det (2 \\pi \\boldsymbol{\\Sigma}) - \\sum\\limits^3_{i=1} \\dfrac{1}{2} \\mathbf{x}_i^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_i$ (answer), where $\\boldsymbol{\\Sigma}$ is a matrix dependent of $\\rho$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$\n",
    "\n",
    "According to the hint, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "\n",
    "Additionally, we know that $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where $\\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix}$, which means that $\\mu_1 = 0, \\mu_2 = 0, \\sigma_1 = 1, \\sigma_2 = 1$. Therefore, the posterior of the unknown variable Z with respect to the 2nd element of the 2nd datapoint, $X_{22}$, is given by\n",
    "\n",
    "$p(Z|X_{22} = x_{22}) \\sim \\mathcal{N}\\left(\\mu_1 + \\dfrac{\\sigma_1}{\\sigma_2}\\rho_c(x_{22}-\\mu_2), (1-\\rho_c^2)\\sigma_1^{2}\\right)$\n",
    "\n",
    "$=> p(Z|X_{22} = x_{22}) \\sim \\mathcal{N}\\left(0 + \\dfrac{1}{1}\\rho_c(x_{22}-0), (1-\\rho_c^2)1^{2}\\right)$\n",
    "\n",
    "$=> p(Z|X_{22} = x_{22}) \\sim \\mathcal{N}\\left(\\rho_c x_{22}, 1-\\rho_c^2\\right)$ (answer), where $\\rho_c$ is the current estimate of the correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ Evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "The expectation of the log likelihood with respect to the posterior distribution of Z is:\n",
    "\n",
    "$\\langle{\\ell(\\rho)}\\rangle = \n",
    "\\left\\langle{-\\dfrac{3}{2} \\log \\det (2 \\pi \\boldsymbol{\\Sigma}) - \\sum\\limits^3_{i=1} \\dfrac{1}{2} \\mathbf{x}_i^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_i}\\right\\rangle$ (result from part (a))\n",
    "\n",
    "Because only $\\mathbf{x}_2$ contains Z, it means the expectation will treat other terms as constant, which looks like this:\n",
    "$\\langle{\\ell(\\rho)}\\rangle = -\\dfrac{3}{2} \\log \\det (2 \\pi \\boldsymbol{\\Sigma})\n",
    "- \\dfrac{1}{2} \\mathbf{x}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_1\n",
    "- \\dfrac{1}{2} \\left\\langle{ \\mathbf{x}_2^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_2}\\right\\rangle\n",
    "- \\dfrac{1}{2} \\mathbf{x}_3^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_3\n",
    "$\n",
    "\n",
    "Therefore, let's focus on calculating only $\\left\\langle{ \\mathbf{x}_2^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_2}\\right\\rangle$, and when we obtain the result, we will plug it into this formula again\n",
    "\n",
    "According to one hint, we have $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = \\left\\langle{ trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T})}\\right\\rangle$, where we can move terms independent of $x_2$ outside the expectation\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\\boldsymbol{\\Sigma}^{-1}\\left\\langle{\\mathbf{x_2x_2^T}}\\right\\rangle \\right)$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\\boldsymbol{\\Sigma}^{-1}\\left\\langle{\\begin{bmatrix} Z \\\\ x_{22} \\end{bmatrix} \\begin{bmatrix} Z & x_{22} \\end{bmatrix}}\\right\\rangle \\right)$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\\boldsymbol{\\Sigma}^{-1}\\left\\langle{\\begin{bmatrix} Z^2 & Zx_{22} \\\\ Zx_{22} & x_{22}^2 \\end{bmatrix}}\\right\\rangle \\right)$\n",
    "\n",
    "Another given hint is that if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$. From part (b), we know that $p(Z|X_{22} = x_{22}) \\sim \\mathcal{N}\\left(\\rho_c x_{22}, 1-\\rho_c^2\\right)$\n",
    "\n",
    "=> $\\langle{Z^2}\\rangle = (\\rho_c x_{22})^2 + (1-\\rho_c^2)$ and $\\langle{Z}\\rangle = \\rho_c x_{22}$. Replace these into the trace equation, the new formula without expectation is derived as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}^{-1}\\begin{bmatrix} (\\rho_c x_{22})^2 + (1-\\rho_c^2) & \\rho_c x_{22}^2 \\\\ \\rho_c x_{22}^2 & x_{22}^2 \\end{bmatrix} \\right)$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\n",
    "\\begin{bmatrix} \\dfrac{1}{1 - \\rho^2} & -\\dfrac{\\rho}{1 - \\rho^2} \\\\ -\\dfrac{\\rho}{1 - \\rho^2} & \\dfrac{1}{1 - \\rho^2} \\end{bmatrix}\n",
    "\\begin{bmatrix} (\\rho_c x_{22})^2 + (1-\\rho_c^2) & \\rho_c x_{22}^2 \\\\ \\rho_c x_{22}^2 & x_{22}^2 \\end{bmatrix} \\right)$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\n",
    "\\dfrac{1}{1-\\rho^2}\n",
    "\\begin{bmatrix} 1 & -\\rho \\\\ \\rho & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} (\\rho_c x_{22})^2 + (1-\\rho_c^2) & \\rho_c x_{22}^2 \\\\ \\rho_c x_{22}^2 & x_{22}^2 \\end{bmatrix} \\right)$\n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = trace\\left(\\dfrac{\\rho_c^2 x_{22}^2 + (1-\\rho_c^2) - 2 \\rho \\rho_c x_{22}^2 + x_{22}^2}{1- \\rho^2}\\right)$\n",
    "\n",
    "Since the whole fraction is scalar, the trace of a scalar is the scalar itself, which means we can remove the trace function. The expectation result is \n",
    "\n",
    "=> $\\left\\langle{\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2}}\\right\\rangle = \\dfrac{\\rho_c^2 x_{22}^2 + (1-\\rho_c^2) - 2 \\rho \\rho_c x_{22}^2 + x_{22}^2}{1- \\rho^2}$ \n",
    "\n",
    "Finally, the expectation of the complete data log likelihood with respect to the posterior distribution of Z is given by: \n",
    "\n",
    "$\\langle{\\ell(\\rho)}\\rangle = -\\dfrac{3}{2} \\log \\det (2 \\pi \\boldsymbol{\\Sigma})\n",
    "\\dfrac{1}{2} \\mathbf{x}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_1\n",
    "\\dfrac{1}{2} \\dfrac{\\rho_c^2 x_{22}^2 + (1-\\rho_c^2) - 2 \\rho \\rho_c x_{22}^2 + x_{22}^2}{1- \\rho^2}\n",
    "\\dfrac{1}{2} \\mathbf{x}_3^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_3\n",
    "$ (answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cd36c403dde3a532a877a43ad92522",
     "grade": false,
     "grade_id": "cell-46bf29d7d4d92271",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 2(a) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Write down the complete data log-likelihood\n",
    "\n",
    "The complete data log-likelihood is the joint log likelihood of both the observed variable, $\\mathbf{x}$ and the latent variable, $\\mathbf{z}$. The log-likelihood formula is:\n",
    "\n",
    "$\\log p(\\mathbf{x}, \\mathbf{z} | \\theta, \\tau) = \\log \\left\\{\\prod\\limits^N_{n=1} p(x_n, z_n| \\theta, \\tau) \\right\\} = \\sum^N_{n=1} \\log p(x_n, z_n | \\theta, \\tau)$\n",
    "\n",
    "=> $\\log p(\\mathbf{x}, \\mathbf{z} | \\theta, \\tau) = \\sum^N_{n=1} \\log \\left(p(x_n | z_n, \\theta) p(z_n | \\tau)\\right) = \\sum^N_{n=1} \\log \\left((1-\\tau)^{z_{n1}} \\mathcal{N}(x_n|0,1)^{z_{n1}} \\times  (1-\\tau)^{z_{n2}} \\mathcal{N}(x_n|\\theta,1)^{z_{n2}}  \\right)$\n",
    "\n",
    "=> $\\log p(\\mathbf{x}, \\mathbf{z} | \\theta, \\tau) = \\sum^N_{n=1} \\left( z_{n1}\\log(1-\\tau) + z_{n1} \\log \\mathcal{N}(x_n|0,1)+ z_{n2} \\log \\tau + z_{n2} \\log \\mathcal{N}(x_n|\\theta, 1) \\right)$ (E.q 1) (answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$\n",
    "\n",
    "#### E-step $1^0$\n",
    "Compute the posterior distribution of the latent variables, given the\n",
    "current estimate $\\theta_0$ of $\\theta$:\n",
    "\n",
    "$p(z_{n1} = 1|x_n,\\theta_0, \\tau_0) \\propto p(z_{n1}=1)p(x_n|z_n, \\theta_0, \\tau_0) = (1 - \\tau_0) \\mathcal{N}(x_n|0,1)$ (E.q 2)\n",
    "\n",
    "$p(z_{n2} = 1|x_n,\\theta_0, \\tau_0) \\propto p(z_{n2}=1)p(x_n|z_n, \\theta_0, \\tau_0) =  \\tau_0 \\mathcal{N}(x_n | \\theta_0, 1) $ (E.q 3)\n",
    "\n",
    "By normalizing these two equations E.q 2 and E.q 3, we get:\n",
    "\n",
    "$ \\gamma(z_{n2}) = p(z_{n2} = 1|x_n,\\theta_0) \\propto \\dfrac{\\tau_0 \\mathcal{N}(x_n | \\theta_0, 1)}\n",
    "{\\tau_0 \\mathcal{N}(x_n | \\theta_0, 1) + (1 - \\tau_0) \\mathcal{N}(x_n | 0, 1)}$ (E.q 4)\n",
    "\n",
    "#### E-step $2^0$\n",
    "Evaluate the expectation of the complete data log-likelihood over the posterior distribution of the latent variables in E.q 4\n",
    "\n",
    "$\\mathcal{Q}(\\theta, \\tau|\\theta_0, \\tau_0) = E_{z|x,\\theta_0, \\tau_0} [\\log p(\\mathbf{x},\\mathbf{z}|\\theta,\\tau))]$\n",
    "\n",
    "$=> \\mathcal{Q}(\\theta, \\tau|\\theta_0, \\tau_0) = \\sum^N_{n=1} \\{ \n",
    "E[z_{n1}] \\log[(1 - \\tau) \\mathcal{N}(x_n|0,1)] + E[z_{n2}] \\log [\\tau \\mathcal{N}(x_n|\\theta, 1)]\n",
    "\\}$\n",
    "\n",
    "$=> \\mathcal{Q}(\\theta, \\tau|\\theta_0, \\tau_0) = \\sum^N_{n=1} \\gamma(z_{n1})\\log(1-\\tau) + \\gamma(z_{n1}) \\log \\mathcal{N}(x_n|0,1)+ \\gamma(z_{n2}) \\log \\tau + \\gamma(z_{n2}) \\log \\mathcal{N}(x_n|\\theta, 1)$ (answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step\n",
    "\n",
    "Maximize $\\mathcal{Q}(\\theta; \\theta_0)$ with respect to $\\theta$. To differentiate $\\mathcal{Q}(\\theta; \\theta_0)$, the following result can be verified by straightforward computation\n",
    "\n",
    "$\\dfrac{d}{d\\theta} \\mathcal{N}(x_n|\\theta,1)=\\mathcal{N}(x_n|\\theta,1)(x_n-\\theta)$\n",
    "\n",
    "Setting $\\dfrac{d}{d\\theta}\\mathcal{Q}(\\theta; \\theta_0)=0 $, we get the result for $\\theta$ \n",
    "\n",
    "$$\\theta = \\dfrac{1}{N_2} \\sum^N_{n=1} \\gamma(z_{n2})x_n \\text{(answer)}$$\n",
    "\n",
    "where we have defined $N_2 = \\sum^N_{n=1} \\gamma(z_{n2})$; which can be interpreted as the\n",
    "effective number of observations assigned to the component 2. Similarly, we can also define  $N_1 = \\sum^N_{n=1} \\gamma(z_{n1})$ for the first component\n",
    "\n",
    "Maximizing for $\\tau$:\n",
    "\n",
    "$\\dfrac{d}{d\\tau} \\mathcal{Q}(\\tau; \\tau_0) = \\sum_n \\left[\\dfrac{\\gamma(z_{n2})}{\\tau} - \\dfrac{\\gamma(z_{n1})}{1 - \\tau}    \\right] = \\dfrac{N_2}{\\tau} - \\dfrac{N_1}{1-\\tau} = 0,$\n",
    "\n",
    "$\\dfrac{d}{d\\tau} \\mathcal{Q}(\\tau; \\tau_0) = \\dfrac{d}{d\\tau} \\sum^N_{n=1} \\gamma(z_{n1})\\log(1-\\tau) + \\gamma(z_{n1}) \\log \\mathcal{N}(x_n|0,1)+ \\gamma(z_{n2}) \\log \\tau + \\gamma(z_{n2}) \\log \\mathcal{N}(x_n|\\theta, 1)$\n",
    "\n",
    "$\\dfrac{d}{d\\tau} \\mathcal{Q}(\\tau; \\tau_0) = \\sum^N_{n=1} - \\dfrac{\\gamma(z_{n1})}{1 - \\tau} + 0 + \\dfrac{\\gamma(z_{n2})}{\\tau} + 0  = \\sum^N_{n=1} - \\dfrac{\\gamma(z_{n1})}{1 - \\tau} + \\dfrac{\\gamma(z_{n2})}{\\tau} = - \\dfrac{N_1}{1-\\tau} + \\dfrac{N_2}{\\tau} $\n",
    "\n",
    "Setting $\\dfrac{d}{d\\tau} \\mathcal{Q}(\\tau; \\tau_0) = 0$, we get the result for $\\tau$ \n",
    "\n",
    "$$- \\dfrac{N_1}{1-\\tau} + \\dfrac{N_2}{\\tau} = 0 => \\tau = \\dfrac{N_2}{N_1+N_2} \\text{ (answer)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b479fd715d72b0417e4f3ac4c0f8e914",
     "grade": false,
     "grade_id": "cell-1abac854e88e7dc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta       tau\n",
      "1.0000000  0.1000000\n",
      "3.2393002  0.3798055\n",
      "3.2787207  0.5396835\n",
      "3.2208416  0.5618515\n",
      "3.2012856  0.5684228\n",
      "3.1949151  0.5705275\n",
      "3.1928396  0.5712102\n",
      "3.1921631  0.5714324\n",
      "3.1919426  0.5715048\n",
      "3.1918707  0.5715284\n",
      "3.1918472  0.5715361\n",
      "3.1918396  0.5715386\n",
      "3.1918371  0.5715394\n",
      "3.1918363  0.5715397\n",
      "3.1918360  0.5715397\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    # r1 = ?\n",
    "    # r2 = ?\n",
    "    \n",
    "    p_zn1_equals_1 = (1 - tau_0) * scipy.stats.norm.pdf(x, 0, 1)\n",
    "    p_zn2_equals_1 = tau_0 * scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "    # This is gamma(z_n2)\n",
    "    r2 = p_zn2_equals_1 / (p_zn1_equals_1 + p_zn2_equals_1)\n",
    "    # This is gamma(z_n2)\n",
    "    r1 = 1 - r2\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    # theta[it] = ?\n",
    "    # tau[it] = ?\n",
    "    N1 = np.sum(r1)\n",
    "    N2 = np.sum(r2)\n",
    "    theta[it] = np.sum(r2*x) / N2\n",
    "    tau[it] = N2/(N1+N2)\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, label = 'theta')\n",
    "plt.plot(range(n_iter), tau, label = 'tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82083e7d021d449db017e6010251e7ad",
     "grade": false,
     "grade_id": "cell-482274cb8fbd6887",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) PyTorch Tensors are multi-dimensional arrays with support for efficient scientific computing operations. They are similar to NumPy arrays, but it can also utilize the graphics processing units (GPUs) for faster computation. PyTorch Tensors can also be used for automatic differentiation, which is essential for training neural networks using techniques such as backpropagation. PyTorch tensors come in different data types such as float, double, and int. Additionally, they can be initialized from existing data sources such as NumPy arrays or Python lists. \n",
    "\n",
    "To run a CPU tensor on GPU in PyTorch, there are several methods:\n",
    "\n",
    "PyTorch provides a simple to use API to transfer the tensor generated on CPU to GPU, which is the .to(device) method, where device is the hardware that Pytorch runs on\n",
    "> device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "> X_train = X_train.to(device)\n",
    "\n",
    "Or we can also set the device parameter in the tensor initialization\n",
    "> device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "> X_train = torch.FloatTensor([0., 1., 2.], device = device)\n",
    "\n",
    "In PyTorch, the torch.cuda package has additional support for CUDA tensor types that implement the same function as CPU tensors, but they utilize GPUs for computation.\n",
    "If we want a tensor to be on GPU, we can call .cuda().\n",
    "> X_train = torch.FloatTensor([0., 1., 2.])\n",
    "> X_train = X_train.cuda() \n",
    "\n",
    "(b) Automatic differentiation is a technique used in optimization mathematics and machine learning for efficiently computing the derivatives of functions defined by computer programs. It is particularly useful for computing gradients, which are used in optimization algorithms such as stochastic gradient descent (SGD) for training machine learning models.\n",
    "\n",
    "Autograd is a package in PyTorch that provides automatic differentiation technique mentioned above. It allows computation of gradients of tensor-valued functions of tensors without the need for explicit differentiation rules or formulas. Autograd works by building a computational graph of the operations performed on the input tensors and then using the chain rule of differentiation to compute the gradients of the output with respect to the inputs.\n",
    "\n",
    "(c) \n",
    "In the 'linear regression with PyTorch' tutorial, the line number indicating the completion of the computation graph is at line 37, while the computation of the gradients is line 41 and update of the weights is on line 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
