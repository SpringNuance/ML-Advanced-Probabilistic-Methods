{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d887fb93479d9888d5cf6be82f0c2e09",
     "grade": false,
     "grade_id": "cell-81c5a400584e4a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (Spring 2023)\n",
    "\n",
    "Pekka Marttinen, Vishnu Raj, Antti Pöllänen, Nikitin Alexander, Sebastiaan De Peuter, Tommi Gröhn, Julien Martinelli, Ali Khoshvishkaie, Onur Poyraz\n",
    "\n",
    "\n",
    "## Exercise 3, due on Tuesday 14th February at 23:50.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Poisson-Gamma\n",
    "2. Problem 2: Multivariate Gaussian\n",
    "3. Problem 3: Posterior of regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bb2e5ebde49e1760a076b099d6e5a6",
     "grade": false,
     "grade_id": "cell-573bbaa2ef327be0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 1: Poisson-Gamma\n",
    "\n",
    "Suppose you have $N$ i.i.d. observations $\\mathbf{x}= \\{x_i\\}_{i=1}^N$ from a $\\operatorname{Poisson}(\\lambda)$ distribution with a rate parameter $\\lambda$ that has a conjugate prior \n",
    "\n",
    "$$\\lambda \\sim \\operatorname{Gamma}(a,b)$$\n",
    "\n",
    "with the shape and rate hyperparameters $a$ and $b$. Derive the posterior distribution $\\lambda|\\bf{x}$.\n",
    "\n",
    "Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```!(imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally! You can use some of the definitions from the exercise description as a reference. The list of valid Latex commands in Jypyter notebook can be found here: http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The Gamma distribution is modeled by either two parameters $\\alpha$ and $\\beta$, or $k$ and $\\theta$. In its PDF, $alpha$ is equal to $k$ and they are called shape parameters. On the other hand, $\\beta$ (rate parameter) is the inverse of $\\theta$ (scale parameter) ($\\beta = \\dfrac{1}{\\theta}$). The PDF of Gamma distribution is:\n",
    "\n",
    "$Gamma(k, \\theta) = \\dfrac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}$ or $Gamma(\\alpha, \\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}$, where $\\Gamma(x)= (x-1)! = \\int^\\infty_0 e^{-t}t^{x-1}dt$  is the gamma function\n",
    "\n",
    "The Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. The Poisson distribution is discrete distribution and is modeled by the $\\lambda$ parameter, which is the expected rate occurences. The PMF of the Poisson distribution is:\n",
    "\n",
    "$Pois(\\lambda) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!}$, where k is the number of occurences (k = 0, 1, 2, etc)\n",
    "\n",
    "In the problem, the $\\lambda$ parameter has a prior of Gamma distribution, which is a conjugate prior for the Poisson likelihood distribution, and the likelihood of random variable i.i.d x is modeled by Pois($\\lambda$). The formula for the posterior of $\\lambda$ given x is defined as:\n",
    "\n",
    "$$p(\\lambda|x) = \\dfrac{p(\\lambda)p(x|\\lambda)}{p(x)} =  \\dfrac{p(\\lambda)p(x|\\lambda)}{\\int p(\\lambda)p(x|\\lambda) d\\lambda} \\text{, where p(x) is the evidence/marginal likelihood} $$\n",
    "\n",
    "Because the Gamma distribution is the conjugate prior for the Poisson distribution, it means the Gamma distribution is the conjugate distribution of the posterior. Therefore, the posterior distribution of $\\lambda$ must be modeled by the Gamma distribution as well like the prior. Consider a new single observation x, the posterior is:\n",
    "\n",
    "$$\n",
    "p(\\lambda|x) =\\dfrac\n",
    "{\\dfrac{\\lambda^x e^{-\\lambda}}{x!} * \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}}   \n",
    "{\\int^\\infty_0 \\dfrac{\\lambda^x e^{-\\lambda}}{x!} * \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda} d\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> p(\\lambda|x) =\\dfrac\n",
    "{\\dfrac{\\lambda^x e^{-\\lambda}}{x!} * \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}}   \n",
    "{\\int^\\infty_0 \\dfrac\n",
    "{\\beta^{\\alpha+x}\\lambda^{\\alpha + x - 1}e^{-\\lambda(\\beta+1)}}\n",
    "{\\beta^x x! \\Gamma(\\alpha)}\n",
    "d\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> p(\\lambda|x) =\\dfrac\n",
    "{\\dfrac{\\lambda^x e^{-\\lambda}}{x!} * \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}}   \n",
    "{\\dfrac{1}{\\beta^x x! \\Gamma(\\alpha)} \\int^\\infty_0 \n",
    "\\beta^{\\alpha+x}\\lambda^{\\alpha + x - 1}e^{-\\lambda(\\beta+1)}\n",
    "d\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> p(\\lambda|x) =\\dfrac\n",
    "{\\dfrac{\\lambda^x e^{-\\lambda}}{x!} * \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta \\lambda}}   \n",
    "{\\dfrac{\\Gamma(\\alpha + x)}{\\beta^x x! \\Gamma(\\alpha)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> p(\\lambda|x) =\\dfrac\n",
    "{\\beta^(\\alpha + x)}\n",
    "{\\Gamma(\\alpha + x)}\n",
    "\\lambda^{\\alpha+x+1} e^{-(\\beta+1)\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=> p(\\lambda|x) = Gamma(\\alpha + x, \\beta + 1)\n",
    "$$\n",
    "\n",
    "Because we have have $N$ i.i.d. observations $\\mathbf{x}= \\{x_i\\}_{i=1}^N$ \n",
    "Since this is proportional to a Gamma distribution, the posterior distribution is:\n",
    "\n",
    "$$\\lambda|\\mathbf{x} \\sim \\operatorname{Gamma}(a + \\sum_{i=1}^N x_i, b + N)$$\n",
    "\n",
    "So, the posterior distribution of $\\lambda$ given the observed data $\\mathbf{x}$ is a Gamma distribution with the shape parameter $a + \\sum_{i=1}^N x_i$ and the rate parameter $b + N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d1bd8470ba33c5aa2596654e3cefbc",
     "grade": false,
     "grade_id": "cell-7fdfccb96ae5c3d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 2: Multivariate Gaussian\n",
    "\n",
    "Suppose we have $N$ i.i.d. observations $\\mathbf{X} = \\{\\mathbf{x}_i\\}_{i=1}^N$ from a multivariate Gaussian distribution $$\\mathbf{x}_i \\mid \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$ with unknown mean parameter $\\boldsymbol{\\mu}$  and a known covariance matrix $\\boldsymbol{\\Sigma}$. As prior information on the mean parameter we have $$ \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\mathbf{m_0}, \\mathbf{S_0}). $$\n",
    "\n",
    "__(a)__ Derive the posterior distribution $p(\\boldsymbol{\\mu}|\\mathbf{X})$ of the mean parameter $\\boldsymbol{\\mu}$. Write your solution in LateX or attach a picture of the solution in the cell below.\n",
    "\n",
    "__(b)__ Compare the Bayesian estimate (posterior mean) to the maximum likelihood estimate by generating $N=10$ observations from the bivariate Gaussian \n",
    "        $$\\mathcal{N}\\left(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\right).$$\n",
    "For this you can use the Python function [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html), making use of the fact that the elements of the bivariate random vectors are independent. In the Bayesian case, use the prior with $\\mathbf{m_0} = [0,0]^T$ and $\\mathbf{S_0} = [\\begin{smallmatrix}0.1 & 0 \\\\ 0 & 0.1\\end{smallmatrix}]$. Report both estimates. Is the Bayesian estimate closer to the true value $\\boldsymbol{\\mu} = [0,0]^T$? Use the code template given below (after the answer cell) to complete your answer.\n",
    "\n",
    "Write your solutions to __(a)__ and __(b)__ in LateX or attach a picture in the answer cell provided below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ Derive the posterior distribution $p(\\boldsymbol{\\mu}|\\mathbf{X})$ of the mean parameter $\\boldsymbol{\\mu}$. Write your solution in LateX or attach a picture of the solution in the cell below.\n",
    "\n",
    "The multivariate Gaussian distribution: \n",
    "$p(x|\\mu,\\Sigma)=\\mathcal{N}(x|\\mu,\\Sigma)=\\dfrac{1}{det(2\\pi\\Sigma)} \\exp\\left(-\\frac{1}{2}(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu)\\right)$\n",
    "\n",
    "The Gaussian prior distribution for the mean is:\n",
    "$p(\\mu|m_0,S_0)=\\mathcal{N}(m_0,S_0)=\\dfrac{1}{det(2\\pi S_0)} \\exp\\left(-\\frac{1}{2}(\\mu-m_0)^T S_0^{-1}(\\mu-m_0)\\right)$\n",
    "\n",
    "It is known the products of Gaussia distributions is also a Gaussian distribution. Particularly, if the resulting distribution is proportional to this expression:\n",
    "\n",
    "$p(\\mu|X) \\propto \\exp(-\\frac{1}{2}x^TAx+b^Tx)$, then it is said to be following the Gaussian distribution with $p(\\mu|X) \\sim  \\mathcal{N}(x|A^{-1}b,A^{-1})$. The proof is via the completing the square technique for Gaussian distribution manipulation. It is therefore preferable to derive an expression looking like that form. \n",
    "\n",
    "The posterior distribution of $\\boldsymbol{\\mu}$ is proportional to the likelihood multiplied by the prior:\n",
    "\n",
    "$p(\\mu|X) \\propto \\sum^N_i p(x_i|\\mu,\\Sigma)p(\\mu|m_0,S_0)\n",
    "$ \n",
    "\n",
    "$=> p(\\mu|X) \\propto \\dfrac{1}{det(2\\pi\\Sigma)} \n",
    "\\exp\\left(-\\frac{1}{2}\\sum^N_i(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu)\\right) \\dfrac{1}{det(2\\pi S_0)} \n",
    "\\exp\\left(-\\frac{1}{2}(\\mu-m_0)^T S_0^{-1}(\\mu-m_0)\\right)\n",
    "$ \n",
    "\n",
    "$=> p(\\mu|X) \\propto \\dfrac{1}{det(2\\pi\\Sigma)} \\dfrac{1}{det(2\\pi S_0)}\n",
    "\\exp\\left(-\\frac{1}{2}\\sum^N_i(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu)\n",
    "          -\\frac{1}{2}(\\mu-m_0)^T S_0^{-1}(\\mu-m_0)\\right)\n",
    "$ \n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}\\sum^N_i(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu)\n",
    "          -\\frac{1}{2}(\\mu-m_0)^T S_0^{-1}(\\mu-m_0)\\right) $ since $\\dfrac{1}{det(2\\pi\\Sigma)} \\dfrac{1}{det(2\\pi S_0)}$ is constant with respect to $\\mu$\n",
    "\n",
    "We need to expand the term $(\\mu-m_0)^T S_0^{-1}(\\mu-m_0)$\n",
    "\n",
    "$(\\mu-m_0)^T S_0^{-1}(\\mu-m_0) = \\mu^T S_0^{-1}\\mu - \\mu^T S_0^{-1} m_0^T - m_0^T S_0^{-1}\\mu + m_0^T S_0^{-1}m_0 \n",
    "= \\mu^T S_0^{-1}\\mu - 2m_0^T S_0^{-1}\\mu + m_0^T S_0^{-1}m_0\n",
    "$. \n",
    "\n",
    "We can do similarly for $(x_i-\\mu)^T \\Sigma^{-1}(x_i-\\mu)$ as well\n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}\\sum^N_i(\\mu^T \\Sigma^{-1}\\mu - 2x_i^T \\Sigma^{-1}\\mu + x_i^T \\Sigma^{-1}x_i)\n",
    "          -\\frac{1}{2}(\\mu^T S_0^{-1}\\mu - 2m_0^T S_0^{-1}\\mu + m_0^T S_0^{-1}m_0)\\right) $\n",
    "          \n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}\\sum^N_i(\\mu^T \\Sigma^{-1}\\mu - 2x_i^T \\Sigma^{-1}\\mu)\n",
    "          -\\frac{1}{2}(\\mu^T S_0^{-1}\\mu - 2m_0^T S_0^{-1}\\mu)\\right) $ since $x_i^T \\Sigma^{-1}x_i$ and $m_0^T S_0^{-1}m_0$ are constant with respect to $\\mu$\n",
    "          \n",
    "Since this term $\\mu^T \\Sigma^{-1}\\mu$ does not depend on $x_i$, it can simply be multiplied by N outside of the sum as follows:\n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}N\\mu^T\\Sigma^{-1}\\mu+\\sum^N_i x_i^T \\Sigma^{-1}\\mu-\\frac{1}{2}\\mu^TS_0^{-1}\\mu+m_0^TS_0^{-1}\\mu\\right) $\n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}(N\\mu^T\\Sigma^{-1}\\mu+\\mu^TS_0^{-1}\\mu)+\\sum^N_i x_i^T \\Sigma^{-1}\\mu+m_0^TS_0^{-1}\\mu\\right) $. Since N is a scalar, we can move it back before $S_0$ and factorize the equation as:\n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}(\\mu^TN\\Sigma^{-1}\\mu+\\mu^TS_0^{-1}\\mu)+\\sum^N_i x_i^T \\Sigma^{-1}\\mu+m_0^TS_0^{-1}\\mu\\right) $. \n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}\\mu^T(N\\Sigma^{-1}+S_0^{-1})\\mu+ (\\sum^N_i x_i^T \\Sigma^{-1} + m_0^TS_0^{-1})\\mu\\right) $. \n",
    "\n",
    "This part $(\\sum^N_i x_i^T \\Sigma^{-1} + m_0^TS_0^{-1})$ can be transposed to be matching the Gaussian distribution expression above. Note: nothing happens to the covariance matrices as they are symmetric (both normal and inverse version)\n",
    "\n",
    "$=> p(\\mu|X) \\propto \n",
    "\\exp\\left(-\\frac{1}{2}\\mu^T(N\\Sigma^{-1}+S_0^{-1})\\mu+ (\\Sigma^{-1} \\sum^N_i x_i + S_0^{-1} m_0)^T\\mu\\right) $. \n",
    "\n",
    "Now we can define $A$ as $A=N\\Sigma^{-1}+S_0^{-1}$ and $b$ as $b = \\Sigma^{-1} \\sum^N_i x_i + S_0^{-1} m_0$. \n",
    "\n",
    "Therefore, the posterior distribution for the parameter $\\mu$ is:\n",
    "\n",
    "$p(\\mu|X) \\sim  \\mathcal{N}(\\mu|\\mu_{post},S_{post})$ where \n",
    "\n",
    "$\\mu_{post} = A^{-1}b = (N\\Sigma^{-1}+S_0^{-1})^{-1} (\\Sigma^{-1} \\sum^N_i x_i + S_0^{-1} m_0)$ \n",
    "\n",
    "and $S_{post} = A^{-1} = (N\\Sigma^{-1}+S_0^{-1})^{-1}$ (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af88913931d4649db8324917756a5b72",
     "grade": false,
     "grade_id": "cell-e6a09ef8bf1f72d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle:  [-0.19678628 -0.59426098]\n",
      "posterior mean:  [-0.09839314 -0.29713049]\n"
     ]
    }
   ],
   "source": [
    "# template for 2(b)\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Prior covariance matrix for μ\n",
    "S0 = np.array([[0.1, 0],[0, 0.1]])\n",
    "\n",
    "# Prior mean vector for X\n",
    "Mu = np.array([0, 0])\n",
    "\n",
    "# Likelihood covarince matrix for X \n",
    "Sigma = np.array([[1, 0],[0, 1]])\n",
    "\n",
    "std = np.array([1,1])\n",
    "\n",
    "# Number of samples\n",
    "N = 10\n",
    "\n",
    "# Sample N bivariate normal vectors\n",
    "# compute MLE and also the posterior mean solution\n",
    "\n",
    "x = np.random.normal(loc=Mu,scale=std, size=(N,2)) #EXERCISE\n",
    "\n",
    "# The maximum likelihood estimates of μ for the multivariate Gaussian distribution is simply the sample mean\n",
    "mle = np.mean(x, axis = 0) #EXERCISE\n",
    "\n",
    "# Formula from part (a)\n",
    "A = N*inv(Sigma) + inv(S0)\n",
    "b = np.dot(inv(Sigma),sum(x)) + np.dot(inv(S0), Mu)\n",
    "\n",
    "posterior_mean = np.dot(inv(A), b) #EXERCISE \n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "print(\"mle: \", mle)\n",
    "print(\"posterior mean: \", posterior_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the Bayesian estimate closer to the true value $\\boldsymbol{\\mu} = [0,0]^T$?\n",
    "\n",
    "Answer: Yes, the Bayesian estimate is consistently closer to the mean than the MLE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf1e85bf2fabec6a07c3676a5945499",
     "grade": false,
     "grade_id": "cell-6f265c79745ea700",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Posterior of regression weights\n",
    "\n",
    "Suppose $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$. Assume a prior $$\\mathbf{w} \\sim \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I}).$$ Use 'completing the square' to show that the posterior of $\\mathbf{w}$ is given by $p(\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{x}, \\alpha, \\beta)=\\mathcal{N}(\\mathbf{w} \\mid \\mathbf{m}, \\mathbf{S}),$ where \n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left( \\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T \\right)^{-1}\\;, \\\\\n",
    "    \\mathbf{m} &= \\beta \\mathbf{S} \\sum_{i=1}^{n} y_i \\mathbf{x}_i.\n",
    "\\end{align*}\n",
    "\n",
    "Write your solution in LateX or attach a picture of the solution in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing the square claim: (already proven in lecture notes). This requires A to be symmetric\n",
    "\n",
    "$$ \\dfrac{1}{2}x^TAx-b^Tx = \\dfrac{1}{2}(x-A^{-1}b)^TA(x-A^{-1}b)-\\dfrac{1}{2}b^TA^{-1}b$$\n",
    "\n",
    "where the Gaussian distribution is constructed as $\\mathcal{N}(x|A^{-1}b, A^{-1})$, because the term $-\\dfrac{1}{2}b^TA^{-1}b$ is constant with respect to $x$\n",
    "\n",
    "Terminology\n",
    "\n",
    "$\\alpha$: precision of the regression weights, which determines the amount of regularization\n",
    "\n",
    "$\\beta$: precision of the noise\n",
    "\n",
    "Altogether, we have $\\Gamma = \\{\\alpha, \\beta\\}$ are called the hyperparameters\n",
    "\n",
    "First, we can inspect this condition: $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$. Because the mean of the noise $\\epsilon_i$ is 0, it means that the regressed values $y_i$ are expected to be equal to $\\mathbf{w}^{T}\\mathbf{x}_{i}$. Furthermore, the noise indicates how much $y_i$ differs from its expected value, which means that the standard deviation of the noise is also the deviation of $y_i$. Therefore, the likelihood normal distribution for $y_i$, given the features x, the weights w and the hyperparameters $\\beta$ is: $p(\\textbf{y}|\\textbf{x}, \\textbf{w}, \\beta) = \\mathcal{N}(\\textbf{y}|\\textbf{X}\\textbf{w}, \\beta^{-1})$\n",
    "\n",
    "We are also given the prior for the weights, which is $\\textbf{w} \\sim \\mathcal{N} (\\textbf{w}, 0,\\alpha^{-1}\\mathbf{I})$. Together, the posterior for the regression weights $\\textbf{w}$ is proportional to: \n",
    "\n",
    "$p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto p(\\textbf{w}|\\alpha) p(\\textbf{y}|\\textbf{x}, \\textbf{w}, \\beta)$\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \\mathcal{N} (\\textbf{w}|0,\\alpha^{-1}\\mathbf{I}) \\mathcal{N}(\\textbf{y}|\\textbf{X}\\textbf{w}, \\beta^{-1})$. Applying the process like exercise (a), we have:\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}(\\textbf{w}-0)^T(\\alpha^{-1}\\textbf{I})^{-1}(\\textbf{w}-0)\\right)   \n",
    "\\exp\\left(-\\dfrac{1}{2}(\\textbf{y}-\\textbf{X}\\textbf{w})^T(\\beta^{-1})^{-1}(\\textbf{y}-\\textbf{X}\\textbf{w})\\right)\n",
    "$\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}\\textbf{w}^T\\alpha\\textbf{w}\n",
    "          -\\dfrac{1}{2}(\\textbf{y}-\\textbf{X}\\textbf{w})^T\\beta(\\textbf{y}-\\textbf{X}\\textbf{w})\\right)\n",
    "$. Reorder the equation, we have:\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}\\beta(\\textbf{y}-\\textbf{X}\\textbf{w})^T(\\textbf{y}-\\textbf{X}\\textbf{w})\n",
    "          -\\dfrac{1}{2}\\alpha\\textbf{w}^T\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "\n",
    "We need to expand the identity $(\\textbf{y}-\\textbf{X}\\textbf{w})^T(\\textbf{y}-\\textbf{X}\\textbf{w})$:\n",
    "\n",
    "$(\\textbf{y}-\\textbf{X}\\textbf{w})^T(\\textbf{y}-\\textbf{X}\\textbf{w}) = \\textbf{y}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\textbf{w}-\\textbf{w}^T\\textbf{X}^T\\textbf{y}+\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{w} = \n",
    "\\textbf{y}^T\\textbf{y} - 2\\textbf{y}^T\\textbf{X}\\textbf{w}+\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{w} \n",
    "$ \n",
    "\n",
    "The term $\\textbf{y}^T\\textbf{y}$ can be dropped as it is constant with respect to  $\\textbf{w}$\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}\\beta(-2\\textbf{y}^T\\textbf{X}\\textbf{w}+\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{w})\n",
    "          -\\dfrac{1}{2}\\alpha\\textbf{w}^T\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(\\beta\\textbf{y}^T\\textbf{X}\\textbf{w}-\\dfrac{1}{2}\\beta\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{w}\n",
    "          -\\dfrac{1}{2}\\alpha\\textbf{w}^T\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "From now, we need to reformat the posterior into the completing the square form $\\dfrac{1}{2}x^TAx-b^Tx $. The elements can be reordered as:\n",
    "\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(\\beta\\textbf{y}^T\\textbf{X}\\textbf{w}-\\dfrac{1}{2}\\textbf{w}^T\\beta\\textbf{X}^T\\textbf{X}\\textbf{w}\n",
    "          -\\dfrac{1}{2}\\textbf{w}^T\\alpha\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}\\textbf{w}^T\\beta\\textbf{X}^T\\textbf{X}\\textbf{w}\n",
    "          -\\dfrac{1}{2}\\textbf{w}^T\\alpha\\textbf{w} + \\beta\\textbf{y}^T\\textbf{X}\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "\n",
    "$=>p(\\textbf{w}|\\textbf{x}, \\textbf{y},\\alpha, \\beta) \\propto \n",
    "\\exp\\left(-\\dfrac{1}{2}\\textbf{w}^T\\left(\\beta\\textbf{X}^T\\textbf{X} +\n",
    "          \\alpha\\textbf{I}\\right)\\textbf{w} + (\\beta\\textbf{X}^T\\textbf{y})^T\\textbf{w}\\right)\n",
    "$\n",
    "\n",
    "Now, we can assign the appropriate mean and deviation of the regression weight posterior according the \"completing the square method\" as:\n",
    "\n",
    "$$A = \\beta\\textbf{X}^T\\textbf{X} + \\alpha\\textbf{I}$$\n",
    "\n",
    "$$b = \\beta\\textbf{X}^T\\textbf{y}$$\n",
    "\n",
    "$$\\textbf{S} = A^{-1} = (\\beta\\textbf{X}^T\\textbf{X} + \\alpha\\textbf{I})^{-1} = \\left( \\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T \\right)^{-1} \\text{(proven)} $$\n",
    "\n",
    "$$\\mathbf{m} = A^{-1}b = \\textbf{S}\\beta\\textbf{X}^T\\textbf{y}     = \\beta \\mathbf{S} \\sum_{i=1}^{n} y_i \\mathbf{x}_i  \\text{ (proven)} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
