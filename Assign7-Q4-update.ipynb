{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc39b8e",
   "metadata": {},
   "source": [
    "# Problem 4: Model selection for GMM with BIC and cross validation\n",
    "\n",
    "In many machine learning applications model selection is crucial. In this exercise, you\n",
    "will practice two common approaches for model selection: \n",
    "\n",
    "- Bayesian Information Criterion (BIC) (as an approximation to ‘Bayesian model selection’) and  \n",
    "\n",
    "- Cross-Validation (as a representative for a predictive model selection criterion).\n",
    "\n",
    "You are given a data set (1000 samples of dimension 2) contained in the file data.pickle, which has been sampled from a Gaussian Mixture Model (GMM) using three classes(the true class labels are given for your convenience, but they should not be used in learning the model).\n",
    "\n",
    "In the given code template below, the data will be divided into training and test sets.\n",
    "\n",
    "**(a)** Complete the functions 'compute_bic' and 'cross_validate'. Use both criteria to select the number of components in the GMM using the training data. Plot the both the BIC and the validation log-likelihoods as a function of the number of components, as well as the data with the best model. Do both methods find a model with three components as the most likely?\n",
    "\n",
    "**(b)** Use the selected models to evaluate the test set log-likelihood.\n",
    "\n",
    "**(c)** Explain briefly the pros and cons of the two approaches and comment which approach you would consider better and why.\n",
    "\n",
    "**Hint**: What is the total number of parameters needed to specify the component\n",
    "means and covariance matrices, and the mixture weights? You will need this\n",
    "number to compute BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Starter code for problem 4. The solution template is in the cell below.\n",
    "# Tools for learning Gaussian mixture models; adapted from the BRMLtoolkit by David Barber.\n",
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import numpy.linalg as LA\n",
    "#import scipy.misc\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def condp(X):\n",
    "    return X / np.sum(X, axis=0)\n",
    "\n",
    "def condexp(logp):\n",
    "    pmax = np.max(logp, axis=0)\n",
    "    P = logp.shape[0]\n",
    "    return condp(np.exp(logp - np.tile(pmax, (P, 1))))\n",
    "\n",
    "def GMMlogp(X, H, P, m, S):\n",
    "    D, N = X.shape  # dimension and number of points\n",
    "\n",
    "    logp = np.zeros((N, H))\n",
    "    for i in range(H):\n",
    "        invSi = LA.inv(S[i,:,:])\n",
    "        sign, logdetSi = LA.slogdet(2 * np.pi * S[i,:,:])\n",
    "\n",
    "        for n in range(N):\n",
    "            v = X[:,n] - m[:,i]\n",
    "            logp[n,i] = -0.5 * (v @ invSi @ v) - 0.5 * logdetSi + np.log(P[i])\n",
    "\n",
    "    return logp\n",
    "\n",
    "\n",
    "\n",
    "# Log Likelihood of data X under a Gaussian Mixture Model\n",
    "#\n",
    "# X : each column of X is a datapoint.\n",
    "# P : mixture coefficients\n",
    "# m : means\n",
    "# S : covariances\n",
    "#\n",
    "# Returns: A list containing the log likelihood for each data point in X\n",
    "def GMMloglik(X, P, m, S):\n",
    "    N = X.shape[1]\n",
    "    H = m.shape[1]\n",
    "\n",
    "    logp = GMMlogp(X, H, P, m, S)\n",
    "    logl = [logsumexp(a=logp[n,:], b=np.ones(H)) for n in range(N)]\n",
    "\n",
    "    return logl\n",
    "\n",
    "# Fit a mixture of Gaussian to the data X using EM\n",
    "#\n",
    "# X : each column of X is a datapoint.\n",
    "# H : number of components of the mixture.\n",
    "# n_iter : number of EM iterations\n",
    "#\n",
    "# Returns: (P, m, S, loglik, phgn)\n",
    "# P : learned mixture coefficients\n",
    "# m : learned means\n",
    "# S : learned covariances\n",
    "# loglik : log likelihood of the learned model\n",
    "# phgn : mixture assignment probabilities\n",
    "def GMMem(X, H, n_iter):\n",
    "    D, N = X.shape  # dimension and number of points\n",
    "\n",
    "    # initialise the centres to random datapoints\n",
    "    r = np.random.permutation(N)\n",
    "    m = X[:, r[:H]]\n",
    "\n",
    "    # initialise the variances to be large\n",
    "    s2 = np.mean(np.diag(np.cov(X)))\n",
    "    S = matlib.tile(s2 * np.eye(D), [H, 1, 1])\n",
    "\n",
    "    # intialise the component probilities to be uniform\n",
    "    P = np.ones(H) / H\n",
    "\n",
    "    for emloop in range(n_iter):\n",
    "        # E-step:\n",
    "        logpold = GMMlogp(X, H, P, m, S)\n",
    "\n",
    "        phgn = condexp(logpold.T)  # responsibilities\n",
    "        pngh = condp(phgn.T)       # membership\n",
    "\n",
    "        # M-step:\n",
    "        for i in range(H):   # now get the new parameters for each component\n",
    "            tmp = (X - np.tile(m[:,i:i+1], N)) * np.tile(np.sqrt(pngh[:,i]), (D,1))\n",
    "            Scand = np.dot(tmp, tmp.T)\n",
    "\n",
    "            if LA.det(Scand) > 0.0001:   # don't accept too low determinant\n",
    "                S[i,:,:] = Scand\n",
    "\n",
    "        m = np.dot(X, pngh)\n",
    "        P = np.sum(phgn, axis=1) / N\n",
    "\n",
    "    logl = [logsumexp(a=logpold[n,:], b=np.ones(H)) for n in range(N)]\n",
    "    logl = np.sum(logl)\n",
    "\n",
    "    return P, m, S, logl, phgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for problem 4\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def compute_bic(Xtrain, totalComponents):\n",
    "    BICs = []\n",
    "    for H in range(1, totalComponents+1):    # number of mixture components\n",
    "        print(\"H: {}\".format(H))\n",
    "\n",
    "        P, m, S, loglik, phgn = GMMem(Xtrain, H, 100)  # fit to data\n",
    "        \n",
    "        # Use BIC formula described in lecture notes (without multiplying with -2)\n",
    "        # numParams = ?     # number of parameters in the model\n",
    "        # BIC = ?           # BIC for the model\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        BICs.append(BIC)\n",
    "    return BICs\n",
    "\n",
    "def cross_validate(Xtrain, totalComponents):\n",
    "    foldCount = 5    # number of folds\n",
    "\n",
    "    loglik = np.zeros((totalComponents, foldCount))\n",
    "\n",
    "    Nlearning = Xtrain.shape[1]\n",
    "    order = np.random.permutation(Nlearning)    # to randomize the sample order\n",
    "\n",
    "    for H in range(1, totalComponents+1):     # number of mixture components\n",
    "        print(\"H: {}\".format(H))\n",
    "\n",
    "        for fold in range(foldCount):    # K-fold cross validation (K=5)\n",
    "            ind = fold * int(Nlearning/foldCount) + np.arange(int(Nlearning/foldCount))\n",
    "            val_indices = order[ind]\n",
    "\n",
    "            training_indices = np.setdiff1d(np.arange(Nlearning), val_indices);\n",
    "\n",
    "            X_train = Xtrain[:,training_indices]  # cv training data\n",
    "            X_val   = Xtrain[:,val_indices]       # cv validation data\n",
    "\n",
    "            # train model\n",
    "            P1, m1, S1, loglik1, phgn1 = GMMem(X_train, H, 100)   # fit model\n",
    "\n",
    "            # Predict using the cv trained model\n",
    "            # logl1 = ?\n",
    "            # loglik[H-1,fold] = ?\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    return loglik\n",
    "\n",
    "def model_selection(Xtrain, totalComponents, criterion_flag):\n",
    "    np.random.seed(0)\n",
    "    if criterion_flag:\n",
    "        print('Computing BIC Score')\n",
    "        scores = compute_bic(Xtrain, totalComponents)\n",
    "        ylabel = \"(BIC)\"\n",
    "    elif not criterion_flag:\n",
    "        print('Computing Cross Validation Score')\n",
    "        scores = cross_validate(Xtrain, totalComponents)\n",
    "        scores = np.mean(scores, axis=1)\n",
    "        ylabel = \"(Cross Validation)\"\n",
    "\n",
    "    # plot the model selection curve\n",
    "    plt.bar(np.arange(1, totalComponents+1), scores)\n",
    "    plt.xlabel('Number of Mixture Components')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title('Model Selection' + ylabel)\n",
    "    plt.show()\n",
    "\n",
    "    # select the number of mixture components which minimizes the model selection criteria\n",
    "    h = np.argmax(scores) + 1\n",
    "    print('Selected Number of Mixture Components = {}'.format(h))\n",
    "    \n",
    "    return h\n",
    "\n",
    "# Now train full model with selected number of mixture components\n",
    "def train_full_model(Xtrain, Xtest, h):\n",
    "    np.random.seed(0)\n",
    "    P, m, S, loglik, phgn = GMMem(Xtrain, h, 100)  # fit to data\n",
    "\n",
    "    # Predict using the full trained model (Use GMMem.GMMloglik)\n",
    "    # logl = ?\n",
    "    # print('Test Data Log Likelihood = {0:f}'.format(?))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Plot the best GMM model\n",
    "    plot_data()\n",
    "\n",
    "    for i in range(h):\n",
    "        dV, E = LA.eig(S[i,:,:])\n",
    "\n",
    "        theta = np.arange(0, 2*np.pi, 0.1)\n",
    "        p = np.sqrt(dV.reshape(D,1)) * [np.cos(theta), np.sin(theta)]\n",
    "        x = (E @ p) + np.tile(m[:,i:i+1], (1, len(theta)))\n",
    "\n",
    "        plt.plot(x[0], x[1], 'r-', linewidth=2)\n",
    "\n",
    "    plt.title('Training data, Test data (in black)')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# load data\n",
    "with open(\"/coursedata/data.pickle\", \"rb\") as f:\n",
    "    X, labels = pickle.load(f)\n",
    "\n",
    "D, N = X.shape   # dimension and number of data points\n",
    "\n",
    "ratio = 0.75\n",
    "train_ind = np.random.choice(N, int(ratio * N), replace=False)   # training data index\n",
    "test_ind = np.setdiff1d(np.arange(N), train_ind)                 # test data index\n",
    "\n",
    "Xtrain = X[:,train_ind]            # training data\n",
    "Xtrain_labels = labels[train_ind]  # training data labels\n",
    "\n",
    "Xtest = X[:,test_ind]            # test data\n",
    "Xtest_labels = labels[test_ind]  # test data labels\n",
    "\n",
    "# plot training and test data\n",
    "def plot_data():\n",
    "    for i in sorted(set(Xtrain_labels)):\n",
    "        X_comp = Xtrain[:, Xtrain_labels == i]\n",
    "        plt.plot(X_comp[0], X_comp[1], '.' + 'brgmcyk'[i-1], markersize=6)\n",
    "\n",
    "    plt.plot(Xtest[0], Xtest[1], 'kd', markersize=4, markeredgewidth=0.5, markerfacecolor=\"None\")\n",
    "\n",
    "plot_data()\n",
    "plt.title('Training data, Test data (in black)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "totalComponents = 5  # max number of mixture components\n",
    "criterion_flag = 0   # 0: cross validation, 1: BIC\n",
    "\n",
    "h = model_selection(Xtrain, totalComponents, criterion_flag)\n",
    "train_full_model(Xtrain, Xtest, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "totalComponents = 5  # max number of mixture components\n",
    "criterion_flag = 1   # 0: cross validation, 1: BIC\n",
    "\n",
    "h = model_selection(Xtrain, totalComponents, criterion_flag)\n",
    "train_full_model(Xtrain, Xtest, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee965f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
