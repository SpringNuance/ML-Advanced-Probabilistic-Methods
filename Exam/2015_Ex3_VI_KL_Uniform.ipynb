{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Variational approximation \n",
    "\n",
    "(A) Compute the Kullback—Leibler divergence $KL(q, p)$ between $q(x)=\\text{Uniform}(x|a,b)$ and $p(x) = N(x|0,1)$. \n",
    "\n",
    "The Kullback-Leibler divergence between two probability distributions $p(x)$ and $q(x)$ is defined as:\n",
    "\n",
    "$KL(q||p)=\\int q(x) \\log \\dfrac{q(x)}{p(x)}​dx$\n",
    "\n",
    "In this case, $q(x)=\\text{Uniform}(x∣a,b)$ and $p(x)=\\mathcal{N}(x∣0,1)$, so we have:\n",
    "\n",
    "$ KL(q||p) = \\int_{a}^{b} \\dfrac{1}{b-a} \\log \\left( \\dfrac{1}{b-a} \\left(\\dfrac{1}{\\sqrt{2\\pi}} \\exp(-\\dfrac{x^2}{2}) \\right)^{-1} \\right) dx $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ KL(q||p) = \\log(b-a) + \\log(\\sqrt{2\\pi}) + \\dfrac{1}{b-a} \\int^b_a \\dfrac{x^2}{2} dx$\n",
    "\n",
    "$ KL(q||p) = \\log(b−a) + \\log(\\sqrt{2\\pi}) + \\dfrac{1}{2(b−a)}​[\\dfrac{x^3}{3}​]^b_a ​$\n",
    "\n",
    "$ KL(q||p) = \\log(b−a) + \\log(\\sqrt{2\\pi}) + \\dfrac{b^3−a^3}{6(b−a)} $\n",
    "\n",
    "$ KL(q||p) = \\log(b−a) + \\log(\\sqrt{2\\pi}) + \\dfrac{(b−a)(b^2+ab+a^2)}{6(b−a)} $\n",
    "\n",
    "$ KL(q||p) = \\log(b−a) + \\log(\\sqrt{2\\pi}) + \\dfrac{b^2+ab+a^2}{6} $ (answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) Approximate the Gaussian $\\mathcal{N}(0,1)$ distribution using a variational approximation with approximating distribution $q(x) =\\text{Uniform}(x|a, b)$. The distribution q has two parameters, a and b, which you have to optimize. \n",
    "\n",
    "To approximate the Gaussian distribution N(0,1) using a variational approximation with approximating distribution q(x)=Uniform(x∣a,b), we need to minimize the KL divergence between the two distributions with respect to the parameters a and b. This can be done by taking the derivative of the KL divergence with respect to a and b, setting them equal to zero, and solving for a and b. However, this optimization problem does not have a closed-form solution, so numerical methods such as gradient descent or Newton’s method must be used to find the optimal values of a and b"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
