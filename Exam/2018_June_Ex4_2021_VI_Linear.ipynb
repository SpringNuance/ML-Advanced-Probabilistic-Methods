{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Variational Bayes\n",
    "\n",
    "Suppose you are given data $(y_n, x_n)$, where $y_n \\in R$ and $x_n \\in R^2$ for all $n=1,...,N$. We model this using a linear regression model\n",
    "\n",
    "$$y_n = ax_{n1} + bx_{n2} + \\epsilon_n, n = 1,...N$$\n",
    "\n",
    "$$ \\epsilon_n \\overset {\\text{i.i.d}} \\sim \\mathcal{N}(0,1) $$\n",
    "\n",
    "Prior distributions for the parameters are\n",
    "\n",
    "$$ a \\sim \\mathcal{N}(0,1), \\text{ and} $$\n",
    "$$ b \\sim \\mathcal{N}(0,1) $$\n",
    "\n",
    "Assume a variational mean-field distribution $q(a,b)=q(a)q(b)$ for the parameters of the model, where the factors are assumed to be of the form\n",
    "\n",
    "$$ q(a) = \\mathcal{N}(a| \\mu_a, \\sigma_a^2) $$\n",
    "$$ q(b) = \\mathcal{N}(b| \\mu_b, \\sigma_b^2) $$\n",
    "\n",
    "Derive the variational update for the factor $q(a)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update of factor $q(a)$. Note that this exercise doesnt need the latent variable representation of the model. We can directly use the joint distribution of the model.\n",
    "\n",
    "Step 1: Write down the log joint distribution of the model based on the priors and likelihoods\n",
    "\n",
    "$$ p(\\mathbf{x}, \\mathbf{y}, a, b) = p(\\mathbf{y} | \\mathbf{x}, a, b) \\log p(a) \\log p(b) $$\n",
    "\n",
    "$$ => \\log p(\\mathbf{x}, \\mathbf{y}, a, b) = \\sum_{n=1}^N [\\log p(y_n | x_n, a, b)] + \\log p(a) + \\log p(b) $$\n",
    "\n",
    "The variance of the error is also the variance of the likelihood for $\\mathbf{y}$\n",
    "\n",
    "Substituting in the expressions for the likelihood and prior distributions, we get:\n",
    "\n",
    "\n",
    "$$ \\log p(\\mathbf{x}, \\mathbf{y}, a, b) = \\sum_{n=1}^N [\\log \\mathcal{N}(y_n|ax_{n1} + bx_{n2}, 1)] + \\log \\mathcal{N}(a|0,1) + \\log \\mathcal{N}(b|0,1) $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: To derive the variational update for the factor $q(a​)$ q(a), we can use the coordinate ascent variational inference (CAVI) algorithm. This involves optimizing the ELBO/minimizes the KL divergence with respect to one factor at a time while holding the others fixed. This is done by calculating the expectation of the logarithm with respect to the variational distributions excluding the current one in consideration, which is $q(a)$\n",
    "\n",
    "$\\log q^*(a) = E_{q(b)} [p(\\mathbf{x}, \\mathbf{y}, a, b)] $\n",
    "\n",
    "=> $\\log q^*(a) = E_{q(b)} [\\sum_{n=1}^N [\\log p(y_n | x_n, a, b)] + \\log p(a) + \\log p(b)] $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep only the terms dependent on $a$. The rest terms are constant with respect to this factor can be added to the constant \"C\"\n",
    "\n",
    "=> $\\log q^*(a) = E_{q(b)} [\\log p(a)] +  E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] + C$\n",
    "\n",
    "=> $\\log q^*(a) = \\log p(a) + E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] + C$ (E.q 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the exercise, we have $p(a) = \\mathcal{N}(0,1)$ as the prior. \n",
    "\n",
    "=> $\\log p(a) = \\log \\mathcal{N}(a|0,1) = -\\dfrac{1}{2} \\log 2 \\pi - \\dfrac{1}{2} \\dfrac{(a-0)^2}{1} = -\\dfrac{1}{2} \\log 2 \\pi - \\dfrac{1}{2} a^2 $\n",
    "\n",
    "We can drop the term that is independent of $a$ \n",
    "\n",
    "=> $\\log p(a) = - \\dfrac{1}{2} a^2 + C $ (E.q 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, we have\n",
    " \n",
    "$E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] = E_{q(b)} \\left[ \\sum^N_{n=1} \\left( -\\dfrac{1}{2} \\log 2 \\pi - \\dfrac{1}{2} (y_n - ax_{n1} - bx_{n2})^2 \\right) \\right]$\n",
    "\n",
    "Dropping all terms not depending on $a$, we have:\n",
    "\n",
    "$E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] = E_{q(b)} \\left[ \\sum^N_{n=1} \\left(- \\dfrac{1}{2} (-2ax_{n1}y_{n} + a^2 x_{n1}^2 + 2ax_{n1}bx_{n2}) \\right) \\right]$\n",
    "\n",
    "$E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] = E_{q(b)} \\left[ \\sum^N_{n=1} \\left(ax_{n1}y_{n} - \\dfrac{1}{2} a^2 x_{n1}^2 - ax_{n1}bx_{n2} \\right) \\right]$\n",
    "\n",
    "$E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] = \\sum^N_{n=1} \\left(ax_{n1}y_{n} - \\dfrac{1}{2} a^2 x_{n1}^2 - ax_{n1} E_{q(b)}\\left[b\\right]x_{n2} \\right) $, where $ E_{q(b)}\\left[b\\right] = 0$ in the prior\n",
    "\n",
    "=> $E_{q(b)} [\\log p(\\mathbf{y}|\\mathbf{x},a,b)] = \\sum^N_{n=1} \\left(ax_{n1}y_{n} - \\dfrac{1}{2} a^2 x_{n1}^2 \\right) $ (E.q 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plugging (2)(3) into equation (1), we have:\n",
    "    \n",
    "$\\log q^*(a) = - \\dfrac{1}{2} a^2 + \\sum^N_{n=1} \\left(ax_{n1}y_{n} - \\dfrac{1}{2} a^2 x_{n1}^2 \\right) + C $\n",
    "\n",
    "=> $\\log q^*(a) = - \\dfrac{1}{2} a^2 + a \\sum^N_{n=1}  x_{n1}y_{n} - \\dfrac{1}{2} a^2 \\sum^N_{n=1} x_{n1}^2  + C $\n",
    "\n",
    "=> $\\log q^*(a) = - \\dfrac{1}{2} a^2 ( \\sum^N_{n=1} x_{n1}^2 + 1) + a \\sum^N_{n=1}  x_{n1}y_{n} + C $\n",
    "\n",
    "=> $\\log q^*(a) = - \\dfrac{1}{2} a^2 ( \\sum^N_{n=1} x_{n1}^2 + 1) + a \\sum^N_{n=1}  x_{n1}y_{n} + C $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Figuring out the closed form solution for the variational update for $q(a)$, if it happens that the prior and the likelihood are conjugate. In this case, both prior and likelihood are Gaussian, so the posterior is also Gaussian. \n",
    "\n",
    "Completing the square form $-\\dfrac{1}{2}x^TAx + b^Tx $\n",
    "\n",
    "If $\\log q^*(a) \\propto -\\dfrac{1}{2}x^TAx + b^Tx$ => $q(a) = \\mathcal{N}(a|m, S)$\n",
    "\n",
    "where $\\textbf{S} = A^{-1}$ and $\\textbf{m} = A^{-1}b$\n",
    "\n",
    "\n",
    "=> $\\log q^*(a) \\propto - \\dfrac{1}{2} a^2 ( \\sum^N_{n=1} x_{n1}^2 + 1) + a \\sum^N_{n=1}  x_{n1}y_{n}  $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have the final update for the factor q(a) as:\n",
    "\n",
    "$q(a) = \\mathcal{N}(a|m_a, s_a^2)$\n",
    "\n",
    "where\n",
    "\n",
    "$m_a = s_a^2 (\\sum^N_{n=1} x_{n1} y_n)$\n",
    "\n",
    "and\n",
    "\n",
    "$s_a^2 = (\\sum^N_{n=1} x_{n1}^2 + 1)^{-1}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose the data x = (x_N)^N_{n=1} are distributed i.i.d as x_n \\sim Exp(\\lambda) and assume prior \\lambda \\sim Gamma(\\alpha, \\beta). Derive marginal likelihood p(x|alpha, beta). Hint: Gamma prior is conjugate to exponential likelihood.\n",
    "\n",
    "$p(x|\\alpha,\\beta) = \\frac{\\Gamma(N+\\alpha)}{(\\beta+\\sum_{n=1}^N x_n)^{N+\\alpha}} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x|\\lambda) = \\prod_{n=1}^N \\lambda e^{-\\lambda x_n} = \\lambda^N e^{-\\lambda \\sum_{n=1}^N x_n}$\n",
    "\n",
    "The prior distribution for \\lambda is given by:\n",
    "\n",
    "$p(\\lambda|\\alpha,\\beta) = \\frac{\\beta\\alpha}{\\Gamma(\\alpha)}\\lambda{\\alpha-1}e^{-\\beta\\lambda}$\n",
    "\n",
    "Using Bayes’ theorem, the posterior distribution for \\lambda is given by:\n",
    "\n",
    "$p(\\lambda|x,\\alpha,\\beta) \\propto p(x|\\lambda)p(\\lambda|\\alpha,\\beta) = \\frac{\\beta\\alpha}{\\Gamma(\\alpha)}\\lambda{N+\\alpha-1}e{-(\\beta+\\sum_{n=1}N x_n)\\lambda}$\n",
    "\n",
    "This is the kernel of a Gamma distribution with parameters N+\\alpha and \\beta+\\sum_{n=1}^N x_n.\n",
    "\n",
    "The marginal likelihood is given by:\n",
    "\n",
    "$p(x|\\alpha,\\beta) = \\int p(x|\\lambda)p(\\lambda|\\alpha,\\beta)d\\lambda$\n",
    "\n",
    "Substituting the expressions for the likelihood and prior, we get:\n",
    "\n",
    "$p(x|\\alpha,\\beta) = \\int \\frac{\\beta\\alpha}{\\Gamma(\\alpha)}\\lambda{N+\\alpha-1}e{-(\\beta+\\sum_{n=1}N x_n)\\lambda} d\\lambda$\n",
    "\n",
    "This integral evaluates to:\n",
    "\n",
    "p(x|\\alpha,\\beta) = \\frac{\\Gamma(N+\\alpha)}{(\\beta+\\sum_{n=1}^N x_n)^{N+\\alpha}} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
