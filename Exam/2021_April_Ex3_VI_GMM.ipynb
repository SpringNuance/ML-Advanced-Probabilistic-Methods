{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(Q3) Variational Inference\n",
    "\n",
    "$p(x_n|\\tau, \\lambda_1, \\lambda_2) = \\tau \\mathcal{N}(x_n|0, \\lambda_1^{-1}) + (1 - \\tau)\\mathcal{N}(0, \\lambda_2^{-1})$\n",
    "\n",
    "We have the following priors on the parameters\n",
    "\n",
    "$\\tau \\sim \\text{Beta}(\\alpha_0, \\alpha_0)$\n",
    "\n",
    "$\\lambda_1 \\sim \\text{Gamma}(a_0, b_0)$\n",
    "\n",
    "$\\lambda_2 \\sim \\text{Gamma}(c_0, d_0)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Define the model using latent variables $\\mathbf{z} = \\{z_i\\}^N_{i=1}$ \n",
    "\n",
    "We formulate the model using latent variables $z_n = (z_{1},...,z_{N})$ which explicitly specify the component\n",
    "responsible for generating observation $x_n$. In detail,\n",
    "\n",
    "$z_n = (z_{n1}, z_{n2})^T = \n",
    "\\begin{cases} \n",
    "(1,0)^T, & (x_n \\text{ is from } \\mathcal{N}(x_n|0,\\lambda_1^{-1})) \\\\\n",
    "(0,1)^T, & (x_n \\text{ is from } \\mathcal{N}(x_n|0,\\lambda_2^{-1}))\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "and place a prior on the latent variables\n",
    "\n",
    "$p(\\mathbf{z}|\\tau) = \\prod^N_{n=1} \\tau^{z_{n1}} (1- \\tau)^{z_{n_2}}$\n",
    "\n",
    "The likelihood in the latent variable model is given by\n",
    "\n",
    "$p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2) = \\prod^N_{n=1} \\mathcal{N}(x_n|0,\\lambda_1^{-1})^{z_{n1}} \\mathcal{N}(x_n|0,\\lambda_2^{-1})^{z_{n2}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution of all observed (${\\mathbf{x}}$) and unobserved variables ($\\mathbf{z}, \\tau, \\lambda_1,\\lambda_2$) factorizes as follows\n",
    "\n",
    "$p(\\mathbf{x},\\mathbf{z},\\tau,\\lambda_1,\\lambda_2) = p(\\tau) p(\\lambda_1) p(\\lambda_2) p(\\mathbf{z}|\\tau) p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)$\n",
    "\n",
    "and the log of the joint distribution can correspondingly be written as\n",
    "\n",
    "$\\log p(\\mathbf{x},\\mathbf{z},\\tau,\\lambda_1,\\lambda_2) = \\log p(\\tau) + \\log p(\\lambda_1) + \\log p(\\lambda_2) + \\log p(\\mathbf{z}|\\tau) + \\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)$\n",
    "\n",
    "We approximate the posterior distribution $p(\\mathbf{z},\\tau,\\lambda_1,\\lambda_2|\\mathbf{x}) $ using the factorized variational distribution $q(\\mathbf{z}) q(\\tau) q(\\lambda_1) q(\\lambda_2)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) Derive the variational update for $\\lambda_2$. You can assume mean-field approximation\n",
    "\n",
    "$q(\\mathbf{z}, \\tau, \\lambda_1, \\lambda_2) = q(\\lambda_1)q(\\lambda_2)q(\\tau)\\prod_n q(z_n)$\n",
    "\n",
    "and assume the other factors are given by\n",
    "\n",
    "$q(\\tau) = Beta(\\tau|\\alpha_n,\\beta_n)$\n",
    "\n",
    "$q(z_{n1}) = Bernoulli(z_{n1}| r_{n1})$ where $r_{n1}$ is the updated responsibility\n",
    "\n",
    "$q(\\lambda_1) = Gamma(\\lambda_1|a_n, b_n)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update of factor $q(\\lambda_2)$\n",
    "\n",
    "$\\log q^*(\\lambda_2) = E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}, \\mathbf{z}, \\tau, \\lambda_1, \\lambda_2)] $\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\tau) + \\log p(\\lambda_1) + \\log p(\\lambda_2) + \\log p(\\mathbf{z}|\\tau) + \\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)] $\n",
    "\n",
    "To derive the variational update for $\\lambda_2$​, we need to find the optimal $q(\\lambda_2​)$ that minimizes the KL divergence between the true posterior $p(z,\\tau,\\lambda_1​,\\lambda_2​∣x)$ and the approximating distribution $q(z)q(\\tau)q(\\lambda_1​)q(\\lambda_2​)$. This can be done by applying the coordinate ascent variational inference (CAVI) algorithm.\n",
    "\n",
    "The CAVI update for $q(\\lambda_2​)$ is given by taking the expectation of the log joint distribution with respect to all other factors and then exponentiating the result. We need to keep only the terms dependent on $\\lambda_2$ (having $\\lambda_2$ in the term). The rest terms are constant with respect to this factor can be added to the constant \"C\"\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\lambda_2)] + E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)] + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = \\log p(\\lambda_2) + E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)] + C$ (E.q 1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the exercise, we have $p(\\lambda_2) = \\text{Gamma}(c_0, d_0)$ as the prior. Expand the Gamma distribution for $\\lambda_2$, we have:\n",
    "\n",
    "$\\log p(\\lambda_2) = \\log \\text{Gamma}(\\lambda_2|c_0, d_0) = \\log \\left[ \\dfrac{{d_0}^{c_0}}{\\Gamma(c_0)} \\lambda_2^{c_0-1} \\exp(-d_0 \\lambda_2) \\right] $\n",
    "\n",
    "=> $\\log p(\\lambda_2) = c_0 \\log d_0 - \\log \\Gamma(c_0) + (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 $ \n",
    "\n",
    "\n",
    "We can drop the term that is independent of $\\lambda_2$ \n",
    "\n",
    "=> $\\log p(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + C$ (E.q 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, we have\n",
    " \n",
    "$E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1,\\lambda_2)] = \\prod^N_{n=1} \\mathcal{N}(x_n|0, \\lambda_1^{-1})^{z_{n1}} \\mathcal{N}(x_n| 0, \\lambda_2^{-1})^{z_{n2}}$\n",
    "    \n",
    "=> $E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1, \\lambda_2)] = E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)}\n",
    "[\\sum^N_{n=1} z_{n1}\\log\\mathcal{N}(x_n|0, \\lambda_1^{-1}) + z_{n2}\\log\\mathcal{N}(x_n| 0, \\lambda_2^{-1})]$. We can drop the term that is independent of $\\lambda_2$, which can be treated as a constant\n",
    "    \n",
    "=> $E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1, \\lambda_2)]  = E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)}\n",
    "[\\sum^N_{n=1} z_{n2}\\log\\mathcal{N}(x_n| 0, \\lambda_2^{-1})] + C$\n",
    "\n",
    "By definition, $E_{q(z_n)} [\\sum^N_{n=1}z_{nk}] = \\sum^N_{n=1}r_{nk}$ is the expected responsibility of component $k$ for observation $x_n$ according to Bernoulli distribution​\n",
    "\n",
    "=> $E_{q(\\mathbf{z})q(\\tau)q(\\lambda_1)} [\\log p(\\mathbf{x}|\\mathbf{z},\\lambda_1, \\lambda_2)]  = \n",
    "\\sum^N_{n=1} r_{n2}\\log\\mathcal{N}(x_n| 0, \\lambda_2^{-1}) + C$ (E.q 3)\n",
    "\n",
    "Plugging (2)(3) into equation (1), we have:\n",
    "    \n",
    "$\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + \\sum^N_{n=1} r_{n2}\\log\\mathcal{N}(x_n| 0, \\lambda_2^{-1}) + C$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + \\sum^N_{n=1} r_{n2}\\log[(2 \\pi \\lambda_2^{-1})^{-1/2} \\exp(\\dfrac{1}{2} (x_n - 0)^2 (\\lambda_2^{-1})^{-1})] + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + \\sum^N_{n=1} r_{n2}\\log[(2 \\pi)^{-1/2} \\lambda_2^{1/2} \\exp(\\dfrac{1}{2} x_n^2 \\lambda_2)] + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + \\sum^N_{n=1} r_{n2} \\left[ -\\dfrac{1}{2} \\log (2 \\pi) + \\dfrac{1}{2} \\log \\lambda_2 + \\dfrac{1}{2} x_n^2 \\lambda_2 \\right] + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 - \\dfrac{N}{2} r_{n2} \\log (2 \\pi) + \\dfrac{N}{2} r_{n2} \\log \\lambda_2 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 \\lambda_2 + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 - \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} \\log (2 \\pi) + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} \\log \\lambda_2 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 \\lambda_2 + C$\n",
    "\n",
    "Removing constant term $\\sum^N_{n=1} r_{n2} \\log (2 \\pi)$:\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 - 1) \\log \\lambda_2 - d_0 \\lambda_2 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} \\log \\lambda_2 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 \\lambda_2 + C$\n",
    "\n",
    "=> $\\log q^*(\\lambda_2) = (c_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} - 1) \\log \\lambda_2 - (d_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 ) \\lambda_2 + C$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> $q^*(\\lambda_2) \\propto \\exp(c_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} - 1) \\log \\lambda_2 - (d_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 ) \\lambda_2$, which resembles the Gamma distribution, as the prior of $\\lambda_2$ is Gamma distribution, which is conjugate to the posterior:\n",
    "\n",
    "$q^∗(\\lambda_2​)=\\text{Gamma}(c_N,d_N​)$\n",
    "\n",
    "where\n",
    "\n",
    "$c_N​ = c_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} $​\n",
    "\n",
    "$d_N​ = d_0 + \\dfrac{1}{2} \\sum^N_{n=1} r_{n2} x_n^2 $​\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
